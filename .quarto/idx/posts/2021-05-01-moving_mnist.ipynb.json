{"title":"Moving MNIST","markdown":{"yaml":{"aliases":["/Pytorch/fastai/cv/2021/05/01/moving_mnist"],"author":"Thomas Capelle","badges":true,"categories":["Pytorch","fastai","cv"],"date":"2021-05-01","description":"How to use fastai to predict the movement of numbers","image":"images/moving_mnist.png","output-file":"2021-05-01-moving_mnist.html","title":"Moving MNIST","toc":true},"headingText":"Building Moving MNIST","containsRefs":false,"markdown":"\n\n\n\n![Segmentation](images/moving_mnist.png)\n\nThis tutorial uses fastai to process sequences of images. In this problem, the model has to predict the future frames of a sequence. We will solve a toy example where MNIST digits are moving on a canvas. This is an `ImageTuple` to `ImageTuple` task.\n- First we will construct a moving MNIST dataset.\n- We will train a simple model to forecast the movent of numbers\n- Finally we will try to make a \"SOTA\" model work\n\n> from MNIST\n\nWe are going to construct the dataset starting from the MNIST dataset available from fastai.\n\nMNIST files are split in a `training` and `testing` folder. We will use the trianing one for our experiments.\n\nwe can look at the first image: \n\nWe will define some constants to work with.\n- `digit_size`: is the resolution of the MNIST images (28x28)\n- `image_size`: is the canvas size (64x64)\n- `step_length`: is the \"speed\" of the moving digits on the canvas\n\nwe first have to create random trayectories of the (28x28) digits on the canvas, we will make them bounce back when they hit a border. We will compute the trayectory of the corner of the digit.\n\nlet's grab a random image from the dataset\n\nwe will directly convert to a tensor, to work on the canvas.\n\nto move the digit, we get one randomly and shift using the random trayectory.\n\nwe can combine multiple digits with different trayectories at once.\n\nWe are going to use the mid level APi, but as we already have a tensor, is very simple.\n\nwe will create a simple function to split our sequence on (x,y) where the first `n_in` frames will serve as input and the last `n_out` frames as target.\n\nas the images are generated on the fly, we pass a list of integers to the `TfmdLists` constructor that will only serve as a counting mechanism.\n\nwe will put everything together into a `DataLoaders` object, and we are ready to train.\n\nas we can see with `one_batch` and `explode_types`, we get 3 images as input, and 3 as output\n\n## Refactor \n> Let's put everything together to train with a large dataset\n\nwe have to make a custom `show_batch` method using the `@typedispatch` decorator to be able to show our `ImageSeq` objects.\n\n## How to build a Model for this task?\n> Trying something simple\n\nAs we saw before, the batch is composed of an `ImageSeq` as input and an `ImageSeq` as output, so we need a model capable of processing this. Let's build something super simple.\n- We already have an image to image fastai model called `DyanmicUnet`\n- This model takes one image, and produces another one.\n- The simplest model would not have temporal capabilities, and only process one image at a time. You encode the first image and decode the first target.\n\nAs you can see, the results is a list of 3 tensors with 200 samples each.\n","srcMarkdownNoYaml":"\n\n\n\n![Segmentation](images/moving_mnist.png)\n\nThis tutorial uses fastai to process sequences of images. In this problem, the model has to predict the future frames of a sequence. We will solve a toy example where MNIST digits are moving on a canvas. This is an `ImageTuple` to `ImageTuple` task.\n- First we will construct a moving MNIST dataset.\n- We will train a simple model to forecast the movent of numbers\n- Finally we will try to make a \"SOTA\" model work\n\n## Building Moving MNIST \n> from MNIST\n\nWe are going to construct the dataset starting from the MNIST dataset available from fastai.\n\nMNIST files are split in a `training` and `testing` folder. We will use the trianing one for our experiments.\n\nwe can look at the first image: \n\nWe will define some constants to work with.\n- `digit_size`: is the resolution of the MNIST images (28x28)\n- `image_size`: is the canvas size (64x64)\n- `step_length`: is the \"speed\" of the moving digits on the canvas\n\nwe first have to create random trayectories of the (28x28) digits on the canvas, we will make them bounce back when they hit a border. We will compute the trayectory of the corner of the digit.\n\nlet's grab a random image from the dataset\n\nwe will directly convert to a tensor, to work on the canvas.\n\nto move the digit, we get one randomly and shift using the random trayectory.\n\nwe can combine multiple digits with different trayectories at once.\n\nWe are going to use the mid level APi, but as we already have a tensor, is very simple.\n\nwe will create a simple function to split our sequence on (x,y) where the first `n_in` frames will serve as input and the last `n_out` frames as target.\n\nas the images are generated on the fly, we pass a list of integers to the `TfmdLists` constructor that will only serve as a counting mechanism.\n\nwe will put everything together into a `DataLoaders` object, and we are ready to train.\n\nas we can see with `one_batch` and `explode_types`, we get 3 images as input, and 3 as output\n\n## Refactor \n> Let's put everything together to train with a large dataset\n\nwe have to make a custom `show_batch` method using the `@typedispatch` decorator to be able to show our `ImageSeq` objects.\n\n## How to build a Model for this task?\n> Trying something simple\n\nAs we saw before, the batch is composed of an `ImageSeq` as input and an `ImageSeq` as output, so we need a model capable of processing this. Let's build something super simple.\n- We already have an image to image fastai model called `DyanmicUnet`\n- This model takes one image, and produces another one.\n- The simplest model would not have temporal capabilities, and only process one image at a time. You encode the first image and decode the first target.\n\nAs you can see, the results is a list of 3 tensors with 200 samples each.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2021-05-01-moving_mnist.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","theme":"flatly","title-block-banner":true,"aliases":["/Pytorch/fastai/cv/2021/05/01/moving_mnist"],"author":"Thomas Capelle","badges":true,"categories":["Pytorch","fastai","cv"],"date":"2021-05-01","description":"How to use fastai to predict the movement of numbers","image":"images/moving_mnist.png","title":"Moving MNIST"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
{"title":"The Devil lives in the details","markdown":{"yaml":{"aliases":["/Pytorch/fastai/2021/02/26/image_resizing"],"author":"Thomas Capelle","badges":true,"categories":["Pytorch","fastai"],"date":"2021-02-26","description":"Resizing method matters...","image":"images/pil2tensor.png","output-file":"2021-02-26-image_resizing.html","title":"The Devil lives in the details","toc":true},"headingText":"A simple example","containsRefs":false,"markdown":"\n\n\n\n![(TL;DR](images/pil2tensor.png)\n\nYesterday I was refactoring some code to put on our production code base. It is a simple image classifier trained with fastai. In our deployment env we are not including fastai as requirements and rely only on pure pytorch to process the data and make the inference. (I am waiting to finally be able to install only the fastai vision part, without the NLP dependencies, this is coming soon, probably in fastai 2.3, at least it is in [Jeremy's roadmap](https://github.com/fastai/fastai/projects/1#card-52606857)). So, I have to make the reading and preprocessing of images as close as possible as fastai `Transform` pipeline, to get accurate model outputs. \n\nAfter converting the transforms to `torchvision.transforms` I noticed that my model performance dropped significantly. Initially I thought that it was fastai's fault, but all the problem came from the new interaction between the `tochvision.io.images.read_image` and the `torchvision.transforms.Resize`. This transform can accept `PIL.Image.Image` or Tensors, in short, the resizing does not produce the same image, one is way softer than the other. The solution was not to use the new Tensor API and just use `PIL` as the image reader.\n\n> TL;DR : torchvision's `Resize` behaves differently if the input is a `PIL.Image` or a torch tensor from `read_image`. Be consistent at training / deploy.\n\nLet's take a quick look on the preprocessing used for training and there corresponding torch version with the new tensor API as shown [here](https://github.com/pytorch/vision/blob/master/examples/python/tensor_transforms.ipynb)\n\nBelow are the versions of fastai, fastcore, torch, and torchvision currently running at the time of writing this:\n\n- `python`       : 3.8.6\n- `fastai`       : 2.2.8\n- `fastcore`     : 1.3.19\n- `torch`        : 1.7.1\n- `torch-cuda`   : 11.0\n- `torchvision`  : 2.2.8: 0.8.2\n\n:::{.callout-note}\n\nYou can easily grab this info from `fastai.test_utils.show_install`\n\n:::\n\n> Let's make a simple classifier on the PETS dataset, for more details this comes from the [fastai tutorial](https://docs.fast.ai/tutorial.vision.html)\n\nlet's grab the data\n\nA learner it is just a wrapper of Dataloaders and the model. We will grab an imagene pretrained `resnet18`, we don't really need to train it to illustrate the problem.\n\nand grab one image (`load_image` comes from fastai and returns a memory loaded `PIL.Image.Image`)\n\nLet's understand what is happening under the hood:\n\nand we can call the prediction using fastai `predict` method, this will apply the same transforms as to the validation set.\n- create PIL image\n- Transform the image to pytorch Tensor\n- Scale values by 255\n- Normalize with imagenet stats\n\ndoing this by hand is extracting the preprocessing transforms:\n\nLet's put all transforms together on a fastcore `Pipeline`\n\nwe can then preprocess the image:\n\nand we get the exact same predictions as before\n\n## Using torchvision preprocessing\n> Now let's try to replace fastai transforms with torchvision\n\nlet's first resize the image, we can do this directly over the `PIL.Image.Image` or using `T.Resize` that works both on `IPIL` images or `Tensor`s\n\nwe can then use `T.ToTensor` this will actually scale by 255 and transform to tensor, it is equivalent to both `ToTensor` + `IntToFloatTensor` from fastai.\n\nthen we have to normalize it:\n\nand we get almost and identical results! ouff.....\n\n## Torchvision new Tensor API\n> Let's try this new Tensor based API that torchvision introduced on `v0.8` then!\n\n`read_image` is pretty neat, it actually read directly the image to a pytorch tensor, so no need for external image libraries. Using this API has many advantages, as one can group the model and part of the preprocessing as whole, and then export to torchscript all together: model + preprocessing, as shown in the example [here](https://github.com/pytorch/vision/blob/master/examples/python/tensor_transforms.ipynb)\n\nwe have to scale it, we have a new transform to do this:\n\nOk, the results is pretty different...\n\nif you trained your model with the old API, reading images using PIL you may find yourself lost as why the models is performing poorly. My classifier was predicting completely the opossite for some images, and that's why I realized that something was wrong! \n\nLet's dive what is happening...\n\n## Comparing Resizing methods\n> T.Resize on PIL image vs Tensor Image\n\nWe will use fastai's `show_images` to make the loading and showing of tensor images easy\n\nLet's zoom and plot\n\nThe `PIL` image is smoother, it is not necesarily better, but it is different. From my testing, for darker images the `PIL` reisze has less moire effect (less noise)\n\n## Extra: What if I want to use OpenCV?\n> A popular choice for pipelines that rely on numpy array transforms, as [Albumnetation](https://github.com/albumentations-team/albumentations/blob/master/docs/index.rst)\n\nopencv opens directly an array\n\nBGR to RGB, and channel first.\n\npretty bad also...\n\n### with `INTER_AREA` flag\n> This method is closer to PIL image resize, as it has a kernel that smooths the image.\n\nkinda of better...\n\n## Speed comparison\n> Let's do some basic performance comparison\n\n:::{.callout-note}\n\nI am using [pillow-simd](https://github.com/uploadcare/pillow-simd) with AVX enabled.\n\n:::\n\n## [Beta] Torchvision 0.10\n> This issue has been partialy solved in the latest release of torchvision\n\nlet's use this image that comes from the [issue](https://github.com/pytorch/vision/issues/2950) on github, it really shows the problem with the non antialiased method on the grey concrete.\n\nremember that `T.ToTensor` here also scales the images by 255. to get values in `[0,1]`\n\nlet's compare the pil vs the tensor antialiased resize:\n\nway better than before.\n\n## Conclusions\n\nIdeally, deploy the model with the exact same transforms as it was validated. Or at least, check that the performance does not degrade.\nI would like to see more consistency between both API in pure pytorch, as the user is pushed to use the new `pillow-free` pipeline, but results are not consistent. Resize is a fundamental part of the image preprocessing in most user cases.\n\n- There is an issue [open](https://github.com/pytorch/vision/issues/2950) on the torchvision github about this.\n- Also one about the difference between PIL and openCV [here](https://github.com/python-pillow/Pillow/issues/2718)\n- Pillow appears to be faster and can open a larger variety of image formats.\n\n~~This was pretty frustrating, as it was not obvious where the model was failing.~~\n\n>Important: It appears that torchvsion 0.10 has solved this issue! This feature is still in beta, and probably the default arg should be `antialias=True`.\n","srcMarkdownNoYaml":"\n\n\n\n![(TL;DR](images/pil2tensor.png)\n\nYesterday I was refactoring some code to put on our production code base. It is a simple image classifier trained with fastai. In our deployment env we are not including fastai as requirements and rely only on pure pytorch to process the data and make the inference. (I am waiting to finally be able to install only the fastai vision part, without the NLP dependencies, this is coming soon, probably in fastai 2.3, at least it is in [Jeremy's roadmap](https://github.com/fastai/fastai/projects/1#card-52606857)). So, I have to make the reading and preprocessing of images as close as possible as fastai `Transform` pipeline, to get accurate model outputs. \n\nAfter converting the transforms to `torchvision.transforms` I noticed that my model performance dropped significantly. Initially I thought that it was fastai's fault, but all the problem came from the new interaction between the `tochvision.io.images.read_image` and the `torchvision.transforms.Resize`. This transform can accept `PIL.Image.Image` or Tensors, in short, the resizing does not produce the same image, one is way softer than the other. The solution was not to use the new Tensor API and just use `PIL` as the image reader.\n\n> TL;DR : torchvision's `Resize` behaves differently if the input is a `PIL.Image` or a torch tensor from `read_image`. Be consistent at training / deploy.\n\nLet's take a quick look on the preprocessing used for training and there corresponding torch version with the new tensor API as shown [here](https://github.com/pytorch/vision/blob/master/examples/python/tensor_transforms.ipynb)\n\nBelow are the versions of fastai, fastcore, torch, and torchvision currently running at the time of writing this:\n\n- `python`       : 3.8.6\n- `fastai`       : 2.2.8\n- `fastcore`     : 1.3.19\n- `torch`        : 1.7.1\n- `torch-cuda`   : 11.0\n- `torchvision`  : 2.2.8: 0.8.2\n\n:::{.callout-note}\n\nYou can easily grab this info from `fastai.test_utils.show_install`\n\n:::\n\n## A simple example\n> Let's make a simple classifier on the PETS dataset, for more details this comes from the [fastai tutorial](https://docs.fast.ai/tutorial.vision.html)\n\nlet's grab the data\n\nA learner it is just a wrapper of Dataloaders and the model. We will grab an imagene pretrained `resnet18`, we don't really need to train it to illustrate the problem.\n\nand grab one image (`load_image` comes from fastai and returns a memory loaded `PIL.Image.Image`)\n\nLet's understand what is happening under the hood:\n\nand we can call the prediction using fastai `predict` method, this will apply the same transforms as to the validation set.\n- create PIL image\n- Transform the image to pytorch Tensor\n- Scale values by 255\n- Normalize with imagenet stats\n\ndoing this by hand is extracting the preprocessing transforms:\n\nLet's put all transforms together on a fastcore `Pipeline`\n\nwe can then preprocess the image:\n\nand we get the exact same predictions as before\n\n## Using torchvision preprocessing\n> Now let's try to replace fastai transforms with torchvision\n\nlet's first resize the image, we can do this directly over the `PIL.Image.Image` or using `T.Resize` that works both on `IPIL` images or `Tensor`s\n\nwe can then use `T.ToTensor` this will actually scale by 255 and transform to tensor, it is equivalent to both `ToTensor` + `IntToFloatTensor` from fastai.\n\nthen we have to normalize it:\n\nand we get almost and identical results! ouff.....\n\n## Torchvision new Tensor API\n> Let's try this new Tensor based API that torchvision introduced on `v0.8` then!\n\n`read_image` is pretty neat, it actually read directly the image to a pytorch tensor, so no need for external image libraries. Using this API has many advantages, as one can group the model and part of the preprocessing as whole, and then export to torchscript all together: model + preprocessing, as shown in the example [here](https://github.com/pytorch/vision/blob/master/examples/python/tensor_transforms.ipynb)\n\nwe have to scale it, we have a new transform to do this:\n\nOk, the results is pretty different...\n\nif you trained your model with the old API, reading images using PIL you may find yourself lost as why the models is performing poorly. My classifier was predicting completely the opossite for some images, and that's why I realized that something was wrong! \n\nLet's dive what is happening...\n\n## Comparing Resizing methods\n> T.Resize on PIL image vs Tensor Image\n\nWe will use fastai's `show_images` to make the loading and showing of tensor images easy\n\nLet's zoom and plot\n\nThe `PIL` image is smoother, it is not necesarily better, but it is different. From my testing, for darker images the `PIL` reisze has less moire effect (less noise)\n\n## Extra: What if I want to use OpenCV?\n> A popular choice for pipelines that rely on numpy array transforms, as [Albumnetation](https://github.com/albumentations-team/albumentations/blob/master/docs/index.rst)\n\nopencv opens directly an array\n\nBGR to RGB, and channel first.\n\npretty bad also...\n\n### with `INTER_AREA` flag\n> This method is closer to PIL image resize, as it has a kernel that smooths the image.\n\nkinda of better...\n\n## Speed comparison\n> Let's do some basic performance comparison\n\n:::{.callout-note}\n\nI am using [pillow-simd](https://github.com/uploadcare/pillow-simd) with AVX enabled.\n\n:::\n\n## [Beta] Torchvision 0.10\n> This issue has been partialy solved in the latest release of torchvision\n\nlet's use this image that comes from the [issue](https://github.com/pytorch/vision/issues/2950) on github, it really shows the problem with the non antialiased method on the grey concrete.\n\nremember that `T.ToTensor` here also scales the images by 255. to get values in `[0,1]`\n\nlet's compare the pil vs the tensor antialiased resize:\n\nway better than before.\n\n## Conclusions\n\nIdeally, deploy the model with the exact same transforms as it was validated. Or at least, check that the performance does not degrade.\nI would like to see more consistency between both API in pure pytorch, as the user is pushed to use the new `pillow-free` pipeline, but results are not consistent. Resize is a fundamental part of the image preprocessing in most user cases.\n\n- There is an issue [open](https://github.com/pytorch/vision/issues/2950) on the torchvision github about this.\n- Also one about the difference between PIL and openCV [here](https://github.com/python-pillow/Pillow/issues/2718)\n- Pillow appears to be faster and can open a larger variety of image formats.\n\n~~This was pretty frustrating, as it was not obvious where the model was failing.~~\n\n>Important: It appears that torchvsion 0.10 has solved this issue! This feature is still in beta, and probably the default arg should be `antialias=True`.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2021-02-26-image_resizing.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","theme":"cosmo","title-block-banner":true,"aliases":["/Pytorch/fastai/2021/02/26/image_resizing"],"author":"Thomas Capelle","badges":true,"categories":["Pytorch","fastai"],"date":"2021-02-26","description":"Resizing method matters...","image":"images/pil2tensor.png","title":"The Devil lives in the details"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}